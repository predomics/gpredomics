# unless specifically indicated all field accept a single value
general:
  seed: 42                                    # used in parent selection, child conception (cross over) and mutation, all of which is single thread
  algo: ga                                    # ga for genetic algorithm, beam for beam algorithm and mcmc for MCMC-based algorithm
  cv: false                                   # should cross-validation be enabled?
  thread_number: 8                            # the number of thread used in feature selection and fit computation
  gpu: false                                   # should Gpredomics use GPU ? (ga and beam only)
  language: bin,ter,pow2,ratio                # possible values are ter,bin,ratio,pow2, see README.md for detail. A comma separated list (no spaces) is accepted, which means the initial population will be split 
  data_type: raw,log,prev                     # possible values are raw,prev,log, see README.md for detail. Same as above, comma separated list is fine.
  epsilon: 1e-5                               # this is only usefull for data_type prevalence (where it is a threshold) or log (where it replaces values below)
  fit: auc                                    # possible values are auc,specificity,sensitivity,mcc,f1_score,g_means (classification), see README.md for details
  k_penalty: 0.0001                           # this penalty is deduced from fit function multiplied by k, the number of variables used in the model
  fr_penalty: 0.0                             # used only when fit is specificity or sensitivity, deduce (1 - symetrical metrics) x fr_penalty to fit   
  n_model_to_display: 30                      # n models to display in the last generation (default to 10, 0 means all models)
  #log_base: ""                               # uncomment to print log and results in log_file_name
  log_level: info                             # possible values are trace, debug, info, warning or error 
  display_level: 2                            # precision in variable display (0=anonymized features, 1=feature line index, 2=feature names (default))
  display_colorful: true                      # should the terminal results be coloured to make them easier to read?  
  keep_trace: false                           # keep this setting to false when using gpredomics as a binary
  #save_exp: exp.mp                           # uncomment to save experiment in timestamp-save_exp, which can be reloaded with --load timestamp-save_exp. Extention should be .json, .mp/.msgpack or .bin

data:
  X: "samples/Qin2014/Xtrain.tsv"             # the features of the train data set 
  y: "samples/Qin2014/Ytrain.tsv"             # the class description of the train data set (0=class 0, 1=class 1 (the class to be predicted), 2=unknown status)
  Xtest: "samples/Qin2014/Xtest.tsv"          # the features of the test data set
  ytest: "samples/Qin2014/Ytest.tsv"          # the class description of the test data set 
  features_in_rows: true                      # are the features arranged in rows (legacy Predomics) or columns (standard in ML)?
  max_features_per_class: 0                   # 0: all significant features ; else first X significant features sorted according to their pvalue/log_abs_bayes_factor
  feature_minimal_prevalence_pct: 10          # per class, e.g. features are retained if any of the class reaches this level
  feature_minimal_feature_value: 1e-4         # features which mean is below that value are discarded
  feature_selection_method: wilcoxon          # possible values are wilcoxon, studentt and bayesian_fisher. wilcoxon is recommanded in most cases.
  feature_maximal_adj_pvalue: 0.1             # features with a corrected p-value (BH) above this threshold will be removed
  feature_minimal_log_abs_bayes_factor: 0.5   # features with a fewer log absolute bayes factor will be removed (bayesian method only)
  inverse_classes: false                      # if true, negative class becomes the objective
  classes:
    - "healthy"
    - "cirrhosis"
    - "unknown"

cv:
  inner_folds: 5                              # number of folds used to penalize overfit if overfit_penalty > 0
  overfit_penalty: 0                          # this penalty is deduced from fit function (fit = fit on k-1 - abs(delta with last fold) * overfit_penalty)
  outer_folds: 5                              # number of folds used for cross-validation (launch algorithm on each k-1 folds then merge Families of Best Models).
  resampling_inner_folds_epochs: 0            # to avoid learning about inner folds, resplit it every x periods.
  fit_on_valid: true                          # if true, FBM is based on validation fold fit favouring generalisation, else on k-1 folds ; DO NOT fit on valid without external validation data
  cv_best_models_ci_alpha: 0.05               # alpha for the Family of Best Models confidence interval based on the best fit on validation fold. Smaller alpha, larger best_model range.

importance:
  compute_importance: false                   # should importance be computed ?  
  n_permutations_oob: 100                     # number of permutations per feature for OOB importance.
  scaled_importance: true                     # scale importance by feature prevalence inside folds.
  importance_aggregation: mean                # aggregation method for importances: "mean" or "median".   

voting:
  vote: false                                 # should voting be activated ?
  use_fbm: true                               # should only "Family of Best Models" be kept (alpha = 0.05) ?
  min_perf: 0.5                               # required sensitivity AND specificity to be judge ; >=0.5 avoid "single-choice oriented" judges 
  min_diversity: 10                           # required diversity between judges
  method: Majority                            # Majority: class 1 if > threshold, else class 0 (if = no classification) VS Consensus: non-classification (rejection) if the threshold is not reached
  method_threshold: 0.5                       # typically 0.5 for majority (for equal distribution | if set to 0, optimisation via Youden's maximum) and typically 1 for consensus, classifing samples on which all experts agree
  threshold_windows_pct: 5                    # majority only: if provided, samples with votes within threshold±threshold_windows_pct% (absolute) are not classified (ex: 10% -> votes in the intervals [40% ; 60%] are unclassified)
  complete_display: false                     # if true, display complete results

ga:
  population_size: 5000                       # the target number of models per generation (NB the real number may be below because of clone removal) 
  max_epochs: 100                             # the maximum number of generation before stopping (note that you can stop manually before sending a kill -1 to the process)
  min_epochs: 1                               # the minimum number of generation to do
  max_age_best_model: 100                     # stoping after min_epochs and before max_epochs will occur only if the best model reaches this age
  kmin: 1                                     # the minimal number of variables used in the initial population
  kmax: 200                                   # the maximum number of variables used in the initial population (setting to 0 will remove any maximum) 
  select_elite_pct: 2                         # the % of best models of previous generation retained: the lower the figure the more elitist you are
  select_niche_pct: 20                        # (optional default to 0) the % of best models of previous generation retained but split per language / data type (enable to maintain competition between language/data types)
  select_random_pct: 10                       # the % of opportunistic models of previous generation retained: this is split between all the languages/data_types present in the previous generation
  mutated_children_pct: 80                    # the % of children submitted to mutation
  mutated_features_pct: 20                    # the % of mutation per "gene" (e.g. potential variable), keep it mind that most mutation are "non sense", e.g. remove a variable
  mutation_non_null_chance_pct: 20            # the % of "sense" mutation (e.g. the likeliness that a mutation may add a new variable)
  forced_diversity_pct: 0                     # if >0%, population is filtered every forced_diversity_epochs according to this value. A feature is considered different if it is present WITH SUCH A SIGN in individual A but not in B.
  forced_diversity_epochs : 10                # if forced_diversity_pct > 0%, the epoch gap between two diversity filters
  random_sampling_pct: 0                      # if >0%, each generation is only fitted on a % of random samples reducing overfitting
  random_sampling_epochs: 0                   # if random_samples>0, the number of epoch during a same randomized dataset is kept 

beam:
  method: LimitedExhaustive                   # LimitedExhaustive: generate all combinations (k out of features_to_keep). ParallelForward: increment each extendable_models of each 1 feature from features_to_keep.
  kmin: 2                                     # the number of variables used in the initial population
  kmax: 100                                   # the maximum number of variables to considere in a single model, the variable count limit for beam algorithm
  best_models_ci_alpha: 1e-5                  # alpha for the family of best model confidence interval based on the best fit. Smaller alpha, larger best_model range.
  max_nb_of_models: 20000                     # Limits the number of features_to_keep at each epoch according to the number of models made possible by them (truncated according to the significiance)

mcmc:
  n_iter: 10000                               # number of MCMC (Markov Chain Monte Carlo) iterations
  n_burn: 5000                                # number of MCMC iterations ignored (typically first half of all iterations)
  lambda: 0.001                               # bayesian prior parameter for coefficients a, b, c 
  nmin: 10                                    # minimum number of features in a model after feature elimination | 0 : keep all features (deactivate SBS search)

gpu:
  fallback_to_cpu: true                       # executes the code on the CPU (integrated graphics) if there is no GPU available (recommanded)
  memory_policy: Strict                       # [Strict: panic if below limits are not available | Adaptive: adjusts below limits if not available | Performance: uses all the available GPU memory regardless of below limits
  max_total_memory_mb: 256                    # limit in mb defining the maximum amount of GPU memory used by all buffers
  max_buffer_size_mb: 128                     # limit in mb defining the maximum amount of GPU memory used by one buffer

experimental:
  threshold_ci: false                         # if true, each Individual has a rejection rate around its threshold based on bootstraps
  threshold_ci_penalty: 0.5                   # penalize ga evolution according to rejection_rate*penalty
  threshold_ci_alpha: 0.05                    # alpha to construct the CI around the threshold       
  threshold_ci_n_bootstrap: 1000              # number of boostrap 
  threshold_ci_frac_bootstrap: 0.643          # should a boostrap based on random draw with discount (on all data, frac=1, Efron method) or a random draw without discount on a random subset (0<frac<1, Politis & Romano method)
  bias_penalty: 0                             # penalize models with specificity/sensitivity < 0.5 by doing fit - (1.0 - bad_metrics) * bias_penalty  
  significance_penalty: 0.05                  # weight λ for significance-aware penalty (higher = stronger).
  significance_penalty_threshold: 0.05        # hinge threshold: penalize with above λ max(0, q − q*) for q-values, or max(0, tBF − |log10(BF)|) for bayesian_fisher.